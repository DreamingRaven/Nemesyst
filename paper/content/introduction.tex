\IEEEraisesectionheading{\section{Introduction}\label{sec:introduction}}

\IEEEPARstart{D}{eep learning} is widely known as the sub-field of machine learning that broadly seeks to learn data representations usually using some form or variant of artificial neural networks, which are intuited by their biological counterparts. Deep learning has two distinct tasks:
\begin{itemize}
    \item Training; where the deep neural network (DNN) learns the data representation through iterative methods, such as stochastic gradient descent. This is a very time complex operation, taking hours, days, weeks depending on the task, implementation and computational power available. This usually occurs at most on one node or a few nodes if we consider a single computer with several CPUs/ GPUs to be a multi-node system, which here we shall not.
    \item Inference; where the DNN model now with its learned weights can be used in one form or another to make predictions. This is usually significantly faster than training as it does not require as many operations, but can still suffer from a similar bottleneck \cite{panda2019high}
\end{itemize}

Both training and inference can be improved by use of 'better' hardware, which here shall mean faster and/or higher quantities of processing, and also distributed/parallel hardware. The issue with current deep learning approaches, besides being a time consuming process overall, is that from data handling to evaluation and deployment there is a limit to how much hardware a single computer can support, which is where distributed training and inference is required. Here parallel shall mean any single system that according to flyns taxonomy is using either:
\begin{itemize}
    \item single instruction, multiple data (SIMD)
    \item multiple instruction, single data (MISD)
    \item multiple instruction, multiple data (MIMD)
\end{itemize}
This is separate from a distributed system, as a distributed system is one where each computing node is a different networked computer, as opposed to a single computer with multiple CPUs/GPUs for computation in parallelisation. Currently the majority of deep learning frameworks are primarily focused and have matured in parallelisation, but they lack or are still in their infancy with regards to distribution, albeit there are other efforts currently emerging to solve this problem with varying success, such as TensorFlow-distribute, NVIDA GPU cloud, or IBMs distributed deep learning. \cite{panda2019high}

Lastly the ability to share data between distributed nodes using these deep learning frameworks is also in its infancy despite the prevalence of databases providing very mature and advanced functionality such as MongoDBs aggregate pipeline. The unification of deep learning interfaces to databases will also aid in reproducibility as it will standardise the message protocol interface (MPI) and describe what data in what manner is used for training, and inference, in MongoDBs case, by using aggregate pipeline json files. Clearly the integration of deep learning into database models is critical for any (near) real-time application such as Netflix, Amazon, Google, as 'rolling' a bespoke data management system or MPI is likely too costly, and would result in inferior performance and capabilities.\par

We seek to contribute a new hybrid parallelisation deep learning framework that facilitates practical application of deep learning on a large scale, while also standardising deep learning such that it can be more easily reproducible and consistent. We briefly also show examples of \pfName in use for national grids demand side response (DSR) using data from both a large chain retailer, and our mock store facilities, which seek to simulate the normal operation of an expected retailer store, and its refrigeration units. \clearpage